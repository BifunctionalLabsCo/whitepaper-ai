# fly.toml app configuration file generated for whitepaper-ai on 2025-09-14T17:59:49+02:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'whitepaper-ai'
primary_region = 'ams'

[build]
  image = 'python:3.9'

[env]
  AZURE_AI_MODEL_NAME = 'Meta-Llama-3.1-405B-Instruct'
  DEVELOPMENT = 'false'
  VITE_API_BASE_URL = 'https://whitepaper-ai.fly.dev'

[[services]]
  protocol = 'tcp'
  internal_port = 10000

  [[services.ports]]
    port = 80
    handlers = ['http']

  [[services.ports]]
    port = 443
    handlers = ['tls', 'http']

[[vm]]
  memory = '1gb'
  cpu_kind = 'shared'
  cpus = 1
